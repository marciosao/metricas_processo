import requests
import pandas as pd
import os
import time

# ConfiguraÃ§Ãµes do Redmine
API_KEY = "fc4613361394fdd0c77c9a7c1cb4312bb8f410a4"
URL_BASE = "http://redmine.prodeb.ba.gov.br/"
PROJECT_ID = "projeto-fiplan" 
START_DATE = "2025-02-01"  # Data mÃ­nima para extraÃ§Ã£o

# ConfiguraÃ§Ã£o do caminho de saÃ­da
OUTPUT_DIR = "TESTES"
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "EXTRACAO.CSV")

# Listas para armazenar os dados
##issues_data = []
journals_data = []
journal_details_data = []

# CabeÃ§alhos da requisiÃ§Ã£o
HEADERS = {
    "X-Redmine-API-Key": API_KEY,
    "Content-Type": "application/json"
}

def get_issues(project_id, start_date):
    """Extrai todos os tickets de um projeto do Redmine a partir de uma data especÃ­fica, com status a cada 10 segundos."""
    issues = []
    offset = 0
    limit = 100  # MÃ¡ximo permitido por requisiÃ§Ã£o
    last_report_time = time.time()  # Marca o tempo inicial

    print("Iniciando extraÃ§Ã£o de tickets...")

    while True:
        params = {
            "project_id": project_id,
            "created_on": f"><{start_date}",  # Filtra tickets criados apÃ³s a data especificada
            "offset": offset,
            "limit": limit,
            "status_id": "*",  # Pega todos os status
        }

        response = requests.get(f"{URL_BASE}/issues.json", headers=HEADERS, params=params)

        if response.status_code != 200:
            print(f"Erro {response.status_code}: {response.text}")
            break

        data = response.json()
        issues.extend(data.get("issues", []))

        # Exibe status da extraÃ§Ã£o a cada 10 segundos
        if time.time() - last_report_time >= 10:
            print(f"Tickets extraÃ­dos atÃ© agora: {len(issues)}. ExtraÃ§Ã£o continua...")
            last_report_time = time.time()  # Atualiza o tempo de referÃªncia

        if len(data.get("issues", [])) < limit:
            break  # Sai do loop se nÃ£o houver mais tickets

        offset += limit

    print(f"ExtraÃ§Ã£o finalizada. Total de tickets extraÃ­dos: {len(issues)}")
    return issues

def get_journals(tickets):
    # Etapa 2: Buscar os journals de cada issue
    for issue in tickets:
        issue_id = issue["id"]
        print(f"ğŸ“¡ Buscando journals para a issue {issue_id}...")

        url = f"{URL_BASE}/issues/{issue_id}.json?include=journals"
        response = requests.get(url, headers=HEADERS)

        if response.status_code != 200:
            print(f"âŒ Erro ao buscar journals da issue {issue_id}: {response.status_code}")
            continue

        issue_data = response.json()
        for journal in issue_data.get("issue", {}).get("journals", []):
            journals_data.append({
                "journal_id": journal["id"],
                "issue_id": issue_id,
                "user_id": journal["user"]["id"],
                "notes": journal.get("notes", ""),
                "created_on": journal["created_on"]
            })

        #time.sleep(1)  # Delay para evitar sobrecarga

    print(f"âœ… Journals extraÃ­dos: {len(journals_data)}")
    df_journals = pd.DataFrame(journals_data)
    df_journals.to_csv("./testes/journals.csv", index=False, encoding="utf-8")

    return df_journals

def get_journals_datils(journals):
    # Etapa 3: Buscar os detalhes de cada journal
    for journal in journals_data:
        journal_id = journal["journal_id"]
        issue_id = journal["issue_id"]
        print(f"ğŸ“¡ Detalhes do journal {journal_id} da issue {issue_id}...")

        url = f"{URL_BASE}/issues/{issue_id}.json?include=journals"
        response = requests.get(url, headers=HEADERS)

        if response.status_code != 200:
            print(f"âŒ Erro ao buscar detalhes do journal {journal_id}: {response.status_code}")
            continue

        issue_data = response.json()
        for journal_entry in issue_data.get("issue", {}).get("journals", []):
            if journal_entry["id"] == journal_id:
                for detail in journal_entry.get("details", []):
                    journal_details_data.append({
                        "journal_id": journal_id,
                        "property": detail["property"],
                        "name": detail.get("name", ""),
                        "old_value": detail.get("old_value", ""),
                        "new_value": detail.get("new_value", "")
                    })

        #time.sleep(1)  # Delay para evitar sobrecarga

    print(f"âœ… Detalhes dos journals extraÃ­dos: {len(journal_details_data)}")
    df_journal_details = pd.DataFrame(journal_details_data)
    df_journal_details.to_csv("./testes/journal_details.csv", index=False, encoding="utf-8")

def save_to_csv(issues):
    """Salva os tickets extraÃ­dos em um arquivo CSV na pasta 'EXTRAÃ‡ÃƒO'."""
    if not issues:
        print("Nenhum ticket encontrado.")
        return

    # Cria a pasta se nÃ£o existir
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # Exclui o arquivo se jÃ¡ existir
    if os.path.exists(OUTPUT_FILE):
        os.remove(OUTPUT_FILE)

    # Converte os dados para DataFrame do Pandas
    ####df = pd.DataFrame(issues, columns=["id", "subject", "created_on","custom_fields","created_on","updated_on","tracker.name","status.name","priority.name","author.name","assigned_to.name","category.name","closed_on"])
        
    ####df = pd.DataFrame(issues)

    df = pd.json_normalize(issues)
    # Lista de colunas a serem removidas no segundo arquivo
    colunas_remover = [
        'description',
        'done_ratio',
        'project.id',
        'project.name',
        'tracker.id',
        'status.id',
        'priority.id',
        'author.id',
        'assigned_to.id',
        'category.id',
        'fixed_version.id',
        'fixed_version.name',
        'closed_on',
        'start date',
        'estimated_hours',
        'due_date',
        'start_date'
    ]

    # Criar um novo DataFrame removendo as colunas especificadas
    df_filtrado = df.drop(columns=[col for col in colunas_remover if col in df.columns], errors='ignore')

    # Salva no CSV
    df_filtrado.to_csv(OUTPUT_FILE, index=False, encoding="utf-8-sig")

    print(f"Arquivo '{OUTPUT_FILE}' gerado com sucesso! ({len(df)} tickets)")

# Chamada das funÃ§Ãµes
tickets = get_issues(PROJECT_ID, START_DATE)

journals = get_journals(tickets)

get_journals_datils(journals)

save_to_csv(tickets)
